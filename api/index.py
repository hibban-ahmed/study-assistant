from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse # New import
from fastapi.staticfiles import StaticFiles # New import
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import requests
import json

load_dotenv()

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- NEW: Mount static files ---
# This tells FastAPI to serve files from the 'static' directory
# when requests come to paths starting with /static/ (e.g., /static/styles.css)
# The 'directory="static"' refers to the 'static' folder at your project root.
app.mount("/static", StaticFiles(directory="static"), name="static")

# --- NEW: Serve index.html at the root URL ---
# When a user visits the root of your application (e.g., https://your-app.vercel.app/),
# this route will serve the index.html file.
@app.get("/")
async def serve_frontend():
    # The path "static/index.html" is relative to your project root.
    return FileResponse("static/index.html")

# Health Check Endpoint
@app.get("/api/health")
async def health_check():
    return {"status": "ok", "message": "FastAPI backend is healthy!"}

# Simple Root Endpoint for Diagnosis (for /api)
@app.get("/api")
async def read_api_root():
    return {"message": "FastAPI backend is running at /api root!"}

# Your original /api/generate POST endpoint (uncommented and restored)
# Make sure to uncomment this and the pydantic/requests/os imports if they were commented out for testing
from pydantic import BaseModel # Ensure this is imported if it was commented out
# from dotenv import load_dotenv # Already imported above
# import requests # Already imported above
# import os # Already imported above

class RequestPayload(BaseModel):
    content: str
    prompt: str
    model: str

@app.post("/api/generate")
async def generate_response(data: RequestPayload):
    input_text = f"Study Content:\n{data.content}\n\nInstruction:\n{data.prompt}"

    response_data = None
    status_code = 500

    if data.model == "gemini":
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="Gemini API key not configured.")

        GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": input_text}]
                }
            ],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 1024
            }
        }

        try:
            response = requests.post(
                GEMINI_API_URL,
                headers={"Content-Type": "application/json"},
                json=payload
            )
            response.raise_for_status()
            response_json = response.json()

            if response_json and response_json.get("candidates"):
                generated_text = response_json["candidates"][0]["content"]["parts"][0]["text"]
                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by Gemini.", "raw_response": response_json}
                status_code = 200
        except requests.exceptions.RequestException as e:
            print(f"Error calling Gemini API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to Gemini API: {e}")
        except KeyError as e:
            print(f"Unexpected Gemini API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected Gemini API response structure. Missing data: {e}. Raw response: {json.dumps(response_json)}")

    elif data.model == "llama":
        api_key = os.getenv("LLAMA_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="LLaMA API key not configured.")

        LLAMA_API_URL = "https://api.llama.fake/generate" # <-- UPDATE THIS WITH YOUR REAL LLAMA API ENDPOINT!

        llama_payload = {
            "prompt": input_text,
            "max_tokens": 500,
            "temperature": 0.7
        }

        try:
            response = requests.post(
                LLAMA_API_URL,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=llama_payload
            )
            response.raise_for_status()
            response_json = response.json()

            if response_json:
                generated_text = "No specific text field found in LLaMA response. Raw: " + json.dumps(response_json)
                if isinstance(response_json, dict):
                    if "generated_text" in response_json:
                        generated_text = response_json["generated_text"]
                    elif "text" in response_json:
                        generated_text = response_json["text"]
                    elif response_json.get("choices") and isinstance(response_json["choices"], list) and response_json["choices"][0].get("text"):
                        generated_text = response_json["choices"][0]["text"]
                elif isinstance(response_json, list) and response_json and isinstance(response_json[0], dict) and "text" in response_json[0]:
                    generated_text = response_json[0]["text"]

                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by LLaMA.", "raw_response": response_json}
                status_code = 200

        except requests.exceptions.RequestException as e:
            print(f"Error calling LLaMA API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to LLaMA API: {e}")
        except KeyError as e:
            print(f"Unexpected LLaMA API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected LLaMA API response structure. Missing data: {e}. Raw response: {json.dumps(response_json)}")

    else:
        raise HTTPException(status_code=400, detail="Invalid model selected. Choose 'gemini' or 'llama'.")

    return {"output": response_data}
