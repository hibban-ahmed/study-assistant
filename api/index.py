from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import requests
import json
from pathlib import Path # New import for robust path handling

load_dotenv()

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# --- Robust Path Resolution ---
# Get the absolute path to the directory where this script (index.py) is located
BASE_DIR = Path(__file__).resolve().parent
# Construct the absolute path to the 'static' directory, assuming it's a sibling of 'api'
STATIC_DIR = BASE_DIR.parent / "static"

# Mount static files
# Use the robustly resolved STATIC_DIR
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")

# Serve index.html at root
@app.get("/")
async def serve_frontend():
    # Use the robustly resolved path for index.html
    file_path = STATIC_DIR / "index.html"
    if not file_path.exists():
        return JSONResponse(
            status_code=500,
            content={"error": "index.html not found on server.", "checked_path": str(file_path.absolute())}
        )
    return FileResponse(file_path)

# Health Check Endpoint
@app.get("/api/health")
async def health_check():
    return {"status": "ok", "message": "FastAPI backend is healthy!"}

# Simple Root Endpoint for Diagnosis (for /api)
@app.get("/api")
async def read_api_root():
    return {"message": "FastAPI backend is running at /api root!"}

# Debug Endpoint for Static Files
@app.get("/api/debug-static")
async def debug_static_files():
    debug_info = {}

    # Check if the static directory exists using the robust path
    if not STATIC_DIR.exists():
        debug_info["static_directory_exists"] = False
        debug_info["message"] = f"Static directory '{STATIC_DIR}' not found."
        return JSONResponse(status_code=404, content=debug_info)

    debug_info["static_directory_exists"] = True
    debug_info["static_directory_absolute_path"] = str(STATIC_DIR.absolute())

    # List contents of the static directory
    try:
        debug_info["static_directory_contents"] = [p.name for p in STATIC_DIR.iterdir()]
    except Exception as e:
        debug_info["list_contents_error"] = str(e)

    # Check for index.html specifically
    index_html_path = STATIC_DIR / "index.html"
    if index_html_path.exists():
        debug_info["index_html_exists"] = True
        debug_info["index_html_absolute_path"] = str(index_html_path.absolute())
        try:
            with open(index_html_path, "r") as f:
                debug_info["index_html_first_lines"] = f.read(500) + "..."
        except Exception as e:
            debug_info["read_index_html_error"] = str(e)
    else:
        debug_info["index_html_exists"] = False
        debug_info["message"] = f"index.html not found inside '{STATIC_DIR}'."

    return JSONResponse(content=debug_info)


# Your original /api/generate POST endpoint
class RequestPayload(BaseModel):
    content: str
    prompt: str
    model: str

@app.post("/api/generate")
async def generate_response(data: RequestPayload):
    input_text = f"Study Content:\n{data.content}\n\nInstruction:\n{data.prompt}"

    response_data = None
    status_code = 500

    if data.model == "gemini":
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="Gemini API key not configured.")

        GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": input_text}]
                }
            ],
            "generationConfig": {
                "temperature": 0.7,
                "maxOutputTokens": 1024
            }
        }

        try:
            response = requests.post(
                GEMINI_API_URL,
                headers={"Content-Type": "application/json"},
                json=payload
            )
            response.raise_for_status()
            response_json = response.json()

            if response_json and response_json.get("candidates"):
                generated_text = response_json["candidates"][0]["content"]["parts"][0]["text"]
                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by Gemini.", "raw_response": response_json}
                status_code = 200
        except requests.exceptions.RequestException as e:
            print(f"Error calling Gemini API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to Gemini API: {e}")
        except KeyError as e:
            print(f"Unexpected Gemini API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected Gemini API response structure. Missing data: {e}. Raw response: {json.dumps(response_json)}")

    elif data.model == "llama":
        api_key = os.getenv("LLAMA_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="LLaMA API key not configured.")

        LLAMA_API_URL = "https://api.llama.fake/generate" # <-- UPDATE THIS WITH YOUR REAL LLAMA API ENDPOINT!

        llama_payload = {
            "prompt": input_text,
            "max_tokens": 500,
            "temperature": 0.7
        }

        try:
            response = requests.post(
                LLAMA_API_URL,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=llama_payload
            )
            response.raise_for_status()
            response_json = response.json()

            if response_json:
                generated_text = "No specific text field found in LLaMA response. Raw: " + json.dumps(response_json)
                if isinstance(response_json, dict):
                    if "generated_text" in response_json:
                        generated_text = response_json["generated_text"]
                    elif "text" in response_json:
                        generated_text = response_json["text"]
                    elif response_json.get("choices") and isinstance(response_json["choices"], list) and response_json["choices"][0].get("text"):
                        generated_text = response_json["choices"][0]["text"]
                elif isinstance(response_json, list) and response_json and isinstance(response_json[0], dict) and "text" in response_json[0]:
                    generated_text = response_json[0]["text"]

                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by LLaMA.", "raw_response": response_json}
                status_code = 200

        except requests.exceptions.RequestException as e:
            print(f"Error calling LLaMA API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to LLaMA API: {e}")
        except KeyError as e:
            print(f"Unexpected LLaMA API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected LLaMA API response structure. Missing data: {e}. Raw response: {json.dumps(response_json)}")

    else:
        raise HTTPException(status_code=400, detail="Invalid model selected. Choose 'gemini' or 'llama'.")

    return {"output": response_data}

# fallback for any undefined routes to serve frontend (optional for client-side routing)
@app.get("/{full_path:path}")
async def catch_all(full_path: str):
    index_path = STATIC_DIR / "index.html"
    if index_path.exists():
        return FileResponse(index_path)
    raise HTTPException(status_code=404, detail="Page not found.")
