from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import requests
import json # Import json for parsing API responses

# Load environment variables from .env file for local development
# This line will be ignored by Vercel in production, as Vercel uses its own env vars.
load_dotenv()

app = FastAPI()

# Configure CORS (Cross-Origin Resource Sharing)
# This allows your frontend (even when served from a different origin locally)
# to make requests to your backend.
# In a production environment, you should replace "*" with your Vercel domain
# for better security (e.g., allow_origins=["https://your-app-name.vercel.app"]).
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # Allows all origins
    allow_credentials=True, # Allow cookies/authentication headers to be sent
    allow_methods=["*"],  # Allows all HTTP methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"],  # Allows all headers
)

# Define the data structure for incoming requests using Pydantic
class RequestPayload(BaseModel):
    content: str
    prompt: str
    model: str # "gemini" or "llama"

# Define the API endpoint that the frontend will call
# The path MUST match the frontend's fetch call and the vercel.json routing.
@app.post("/api/generate")
async def generate_response(data: RequestPayload):
    # Combine the study content and prompt into a single input for the AI model
    input_text = f"Study Content:\n{data.content}\n\nInstruction:\n{data.prompt}"

    response_data = None # To store the AI's output
    status_code = 500    # Default error status code

    # --- Gemini API Integration ---
    if data.model == "gemini":
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            # Raise an HTTP exception if the API key is not found
            raise HTTPException(status_code=500, detail="Gemini API key not configured on the server.")

        # Google Gemini API endpoint (using gemini-2.0-flash model)
        # Ensure you have access to this model and endpoint.
        # Check Google Cloud documentation for the latest endpoint and model names.
        GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

        # Gemini API payload structure
        # This sends the combined text as a user message.
        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": input_text}]
                }
            ],
            # Optional: Configure generation parameters like temperature and max tokens
            "generationConfig": {
                "temperature": 0.7, # Controls randomness/creativity (0.0 to 1.0)
                "maxOutputTokens": 1024 # Maximum number of tokens to generate
            }
        }

        try:
            # Make the POST request to the Gemini API
            response = requests.post(
                GEMINI_API_URL,
                headers={"Content-Type": "application/json"},
                json=payload
            )
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)

            response_json = response.json()

            # Parse the generated text from the Gemini response
            # The structure is typically response_json["candidates"][0]["content"]["parts"][0]["text"]
            if response_json and response_json.get("candidates"):
                generated_text = response_json["candidates"][0]["content"]["parts"][0]["text"]
                response_data = {"text": generated_text}
                status_code = 200
            else:
                # If no candidates or text found, return a message with raw response for debugging
                response_data = {"text": "No content generated by Gemini.", "raw_response": response_json}
                status_code = 200 # Still a successful call to Gemini, but no text generated

        except requests.exceptions.RequestException as e:
            # Catch network-related errors (e.g., connection refused, timeout)
            print(f"Error calling Gemini API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to Gemini API: {e}")
        except KeyError as e:
            # Catch errors if the response structure is unexpected
            print(f"Unexpected Gemini API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected Gemini API response structure. Missing data: {e}. Raw response: {json.dumps(response_json)}")

    # --- LLaMA API Integration (Placeholder) ---
    elif data.model == "llama":
        api_key = os.getenv("LLAMA_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="LLaMA API key not configured on the server.")

        # IMPORTANT: REPLACE WITH YOUR ACTUAL LLAMA API INTEGRATION DETAILS
        # The LLaMA API endpoint and payload structure will vary significantly
        # depending on which LLaMA service you are using (e.g., Hugging Face Inference API,
        # Replicate, self-hosted LLaMA, etc.).
        # You MUST consult the documentation for your chosen LLaMA provider.

        LLAMA_API_URL = "https://api.llama.fake/generate" # <-- UPDATE THIS WITH YOUR REAL LLAMA API ENDPOINT!

        # Example placeholder payload for a generic LLaMA API.
        # Adjust 'prompt', 'max_tokens', 'temperature', etc., as per your LLaMA API's requirements.
        llama_payload = {
            "prompt": input_text,
            "max_tokens": 500,
            "temperature": 0.7
        }

        try:
            response = requests.post(
                LLAMA_API_URL,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=llama_payload
            )
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
            response_json = response.json()

            # IMPORTANT: ADJUST THIS RESPONSE PARSING FOR YOUR LLAMA API
            # This is a generic attempt to extract text. Your LLaMA API's response
            # might have a different structure (e.g., response_json["generated_text"],
            # response_json["choices"][0]["text"], etc.)
            if response_json:
                generated_text = "No specific text field found in LLaMA response. Raw: " + json.dumps(response_json)
                # Common patterns for LLaMA-like API responses:
                if isinstance(response_json, dict):
                    if "generated_text" in response_json:
                        generated_text = response_json["generated_text"]
                    elif "text" in response_json:
                        generated_text = response_json["text"]
                    elif response_json.get("choices") and isinstance(response_json["choices"], list) and response_json["choices"][0].get("text"):
                        generated_text = response_json["choices"][0]["text"]
                elif isinstance(response_json, list) and response_json and isinstance(response_json[0], dict) and "text" in response_json[0]:
                    generated_text = response_json[0]["text"]

                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by LLaMA.", "raw_response": response_json}
                status_code = 200

        except requests.exceptions.RequestException as e:
            print(f"Error calling LLaMA API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to LLaMA API: {e}")
        except KeyError as e:
            print(f"Unexpected LLaMA API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected LLaMA API response structure. Missing data: {e}. Raw response: {json.dumps(response_json)}")

    # Handle invalid model selection
    else:
        raise HTTPException(status_code=400, detail="Invalid model selected. Choose 'gemini' or 'llama'.")

    # Return the AI's output
    return {"output": response_data}

