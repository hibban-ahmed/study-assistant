from fastapi import FastAPI, Request, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import os
from dotenv import load_dotenv
import requests
import json # Import json for parsing API responses

# Load environment variables from .env file
load_dotenv()

app = FastAPI()

# Enable CORS for all origins. In a production environment, you might want to restrict this
# to your specific frontend domain for security reasons.
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"], # Allows all origins
    allow_credentials=True,
    allow_methods=["*"], # Allows all methods (GET, POST, PUT, DELETE, etc.)
    allow_headers=["*"], # Allows all headers
)

# Define the request payload structure using Pydantic
class RequestPayload(BaseModel):
    content: str
    prompt: str
    model: str

@app.post("/api/generate") # Changed endpoint to /api/generate to match vercel.json route
async def generate_response(data: RequestPayload):
    # Combine content and prompt into a single input for the AI model
    input_text = f"Study Content:\n{data.content}\n\nInstruction:\n{data.prompt}"

    response_data = None
    status_code = 500 # Default error status code

    if data.model == "gemini":
        api_key = os.getenv("GEMINI_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="Gemini API key not configured.")

        # Google Gemini API endpoint (using gemini-2.0-flash model)
        # Ensure you have access to this model and endpoint.
        GEMINI_API_URL = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"

        # Gemini API payload structure
        payload = {
            "contents": [
                {
                    "role": "user",
                    "parts": [{"text": input_text}]
                }
            ],
            "generationConfig": {
                "temperature": 0.7, # Adjust creativity (0.0 - 1.0)
                "maxOutputTokens": 1024 # Max tokens in the response
            }
        }

        try:
            # Make the POST request to the Gemini API
            response = requests.post(
                GEMINI_API_URL,
                headers={"Content-Type": "application/json"},
                json=payload
            )
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
            response_json = response.json()

            # Extract the generated text from the Gemini response
            if response_json and response_json.get("candidates"):
                # Access the first candidate's content and then the first part's text
                generated_text = response_json["candidates"][0]["content"]["parts"][0]["text"]
                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by Gemini.", "raw_response": response_json}
                status_code = 200 # Still a successful call to Gemini, but no text generated
        except requests.exceptions.RequestException as e:
            print(f"Error calling Gemini API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to Gemini API: {e}")
        except KeyError as e:
            print(f"Unexpected Gemini API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected Gemini API response structure. Missing data: {e}")

    elif data.model == "llama":
        api_key = os.getenv("LLAMA_API_KEY")
        if not api_key:
            raise HTTPException(status_code=500, detail="LLaMA API key not configured.")

        # --- IMPORTANT: REPLACE WITH YOUR ACTUAL LLAMA API INTEGRATION ---
        # The LLaMA API endpoint and payload structure will vary significantly
        # depending on which LLaMA service you are using (e.g., Hugging Face, Replicate, etc.).
        # You will need to consult the documentation for your chosen LLaMA provider.
        LLAMA_API_URL = "https://api.llama.fake/generate" # Placeholder: UPDATE THIS!

        # Example placeholder payload for a generic LLaMA API
        llama_payload = {
            "prompt": input_text,
            "max_tokens": 500,
            "temperature": 0.7
        }

        try:
            response = requests.post(
                LLAMA_API_URL,
                headers={"Authorization": f"Bearer {api_key}", "Content-Type": "application/json"},
                json=llama_payload
            )
            response.raise_for_status() # Raise an HTTPError for bad responses (4xx or 5xx)
            response_json = response.json()

            # --- IMPORTANT: ADJUST RESPONSE PARSING FOR YOUR LLAMA API ---
            # This is a generic attempt to get text. Your LLaMA API's response
            # might have a different structure (e.g., response_json["generated_text"],
            # response_json["choices"][0]["text"], etc.)
            if response_json:
                # Attempt to find text in common LLaMA API response structures
                if isinstance(response_json, dict) and "generated_text" in response_json:
                    generated_text = response_json["generated_text"]
                elif isinstance(response_json, list) and response_json:
                    generated_text = response_json[0].get("text") or json.dumps(response_json[0])
                elif isinstance(response_json, dict) and response_json.get("choices") and response_json["choices"][0].get("text"):
                    generated_text = response_json["choices"][0]["text"]
                else:
                    generated_text = json.dumps(response_json) # Fallback to dumping raw JSON

                response_data = {"text": generated_text}
                status_code = 200
            else:
                response_data = {"text": "No content generated by LLaMA.", "raw_response": response_json}
                status_code = 200
        except requests.exceptions.RequestException as e:
            print(f"Error calling LLaMA API: {e}")
            raise HTTPException(status_code=500, detail=f"Failed to connect to LLaMA API: {e}")
        except KeyError as e:
            print(f"Unexpected LLaMA API response structure: {response_json} - missing key {e}")
            raise HTTPException(status_code=500, detail=f"Unexpected LLaMA API response structure. Missing data: {e}")

    else:
        raise HTTPException(status_code=400, detail="Invalid model selected")

    return {"output": response_data}
